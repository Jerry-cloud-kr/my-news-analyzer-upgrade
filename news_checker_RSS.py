import streamlit as st
import pandas as pd
import os
from newspaper import Article
from sentence_transformers import SentenceTransformer, util # <<--- ì£¼ì„ í•´ì œ
import openai
from openai import OpenAI
import google.generativeai as genai
import feedparser

# --- OpenAI API Key ë° í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (Secrets ì‚¬ìš©) ---
client_openai = None 
OPENAI_API_KEY_Direct_Placeholder = "YOUR_OPENAI_KEY_PLACEHOLDER" 

try:
    OPENAI_API_KEY_FROM_SECRETS = st.secrets["OPENAI_API_KEY"]
    if not OPENAI_API_KEY_FROM_SECRETS:
         st.error("âš ï¸ OpenAI API í‚¤ê°€ Streamlit Secretsì— ì„¤ì •ë˜ì—ˆìœ¼ë‚˜ ê°’ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì•± ì„¤ì •ì—ì„œ í™•ì¸í•´ì£¼ì„¸ìš”.")
         st.stop()
    client_openai = OpenAI(api_key=OPENAI_API_KEY_FROM_SECRETS) 
except KeyError:
    if OPENAI_API_KEY_Direct_Placeholder == "YOUR_OPENAI_KEY_PLACEHOLDER" or not OPENAI_API_KEY_Direct_Placeholder:
        st.error("OpenAI API í‚¤ë¥¼ Streamlit Secretsì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¡œì»¬ í…ŒìŠ¤íŠ¸ë¥¼ ì›í•˜ì‹œë©´ ì½”ë“œ ìƒë‹¨ì˜ OPENAI_API_KEY_Direct_Placeholder ê°’ì„ ì‹¤ì œ í‚¤ë¡œ ì…ë ¥í•˜ê±°ë‚˜, ì•± ë°°í¬ í›„ Streamlit Community Cloudì˜ Secrets ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()
    else: 
        st.warning("ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš© OpenAI API í‚¤ê°€ ì½”ë“œì— ì§ì ‘ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. GitHubì— ë°°í¬/í‘¸ì‹œí•˜ê¸° ì „ì— ì´ ë¶€ë¶„ì„ ë°˜ë“œì‹œ Streamlit Secrets ë°©ì‹ìœ¼ë¡œ ë³€ê²½í•˜ê±°ë‚˜ í‚¤ë¥¼ ì‚­ì œí•˜ì„¸ìš”.", icon="â—")
        client_openai = OpenAI(api_key=OPENAI_API_KEY_Direct_Placeholder)
except Exception as e:
    st.error(f"OpenAI API í‚¤ ì„¤ì • ë˜ëŠ” í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
    st.stop()

if client_openai is None: 
    st.error("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# --- Google AI API Key ì„¤ì • (Secrets ì‚¬ìš©) ---
GOOGLE_AI_API_KEY_Direct_Placeholder = "YOUR_GOOGLE_AI_KEY_PLACEHOLDER" 
try:
    GOOGLE_AI_API_KEY_FROM_SECRETS = st.secrets["GOOGLE_AI_API_KEY"]
    if not GOOGLE_AI_API_KEY_FROM_SECRETS:
         st.error("âš ï¸ Google AI API í‚¤ê°€ Streamlit Secretsì— ì„¤ì •ë˜ì—ˆìœ¼ë‚˜ ê°’ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì•± ì„¤ì •ì—ì„œ í™•ì¸í•´ì£¼ì„¸ìš”.")
         st.stop()
    genai.configure(api_key=GOOGLE_AI_API_KEY_FROM_SECRETS)
except KeyError:
    if GOOGLE_AI_API_KEY_Direct_Placeholder == "YOUR_GOOGLE_AI_KEY_PLACEHOLDER" or not GOOGLE_AI_API_KEY_Direct_Placeholder:
        st.error("Google AI API í‚¤ë¥¼ Streamlit Secretsì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¡œì»¬ í…ŒìŠ¤íŠ¸ë¥¼ ì›í•˜ì‹œë©´ ì½”ë“œ ìƒë‹¨ì˜ GOOGLE_AI_API_KEY_Direct_Placeholder ê°’ì„ ì‹¤ì œ í‚¤ë¡œ ì…ë ¥í•˜ê±°ë‚˜, ì•± ë°°í¬ í›„ Streamlit Community Cloudì˜ Secrets ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()
    else: 
        st.warning("ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš© Google AI API í‚¤ê°€ ì½”ë“œì— ì§ì ‘ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. GitHubì— ë°°í¬/í‘¸ì‹œí•˜ê¸° ì „ì— ì´ ë¶€ë¶„ì„ ë°˜ë“œì‹œ Streamlit Secrets ë°©ì‹ìœ¼ë¡œ ë³€ê²½í•˜ê±°ë‚˜ í‚¤ë¥¼ ì‚­ì œí•˜ì„¸ìš”.", icon="â—")
        genai.configure(api_key=GOOGLE_AI_API_KEY_Direct_Placeholder)
except Exception as e:
    st.error(f"Google AI API í‚¤ ì„¤ì • ì¤‘ ì˜¤ë¥˜: {e}")
    st.stop()

# --- AI ê¸°ëŠ¥ í•¨ìˆ˜ë“¤ ---
def summarize_text_gemini(text_content):
    model = genai.GenerativeModel(
        model_name='gemini-1.5-flash-latest',
        system_instruction="ë„ˆëŠ” ë‰´ìŠ¤ ê¸°ì‚¬ì˜ í•µì‹¬ ë‚´ìš©ì„ ê°ê´€ì ìœ¼ë¡œ ìš”ì•½í•˜ëŠ” AIì•¼."
    )
    prompt = f"ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°ê´€ì ì¸ ì‚¬ì‹¤ì— ê¸°ë°˜í•˜ì—¬ í•µì‹¬ ë‚´ìš© ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì‹­ì‹œì˜¤. ìš”ì•½ì—ëŠ” ì£¼ìš” ì¸ë¬¼, ë°œìƒí•œ ì‚¬ê±´, ì¤‘ìš”í•œ ë°œì–¸, ê·¸ë¦¬ê³  ì‚¬ê±´ì˜ ë°°ê²½ ì •ë³´ê°€ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì£¼ê´€ì ì¸ í•´ì„, í‰ê°€, ë˜ëŠ” ê¸°ì‚¬ì— ëª…ì‹œì ìœ¼ë¡œ ë“œëŸ¬ë‚˜ì§€ ì•Šì€ ì¶”ë¡ ì€ ë°°ì œí•˜ê³ , ì‚¬ì‹¤ ê´€ê³„ë¥¼ ëª…í™•íˆ ì „ë‹¬í•˜ëŠ” ë° ì§‘ì¤‘í•´ ì£¼ì‹­ì‹œì˜¤. ë¶„ëŸ‰ì€ í•œêµ­ì–´ ê¸°ì¤€ìœ¼ë¡œ ì•½ 3~5ë¬¸ì¥ (ë˜ëŠ” 100~150 ë‹¨ì–´) ì •ë„ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ ì£¼ì‹­ì‹œì˜¤.\n\nê¸°ì‚¬:\n{text_content}"
    try:
        response = model.generate_content(prompt,generation_config=genai.types.GenerationConfig(temperature=0.3))
        return response.text.strip()
    except Exception as e:
        st.warning("ìš”ì•½ ìƒì„± ì¤‘ ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        print(f"Gemini ìš”ì•½ API ì˜¤ë¥˜: {e}")
        return "ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

def detect_bias_openai(title, text_content):
    prompt = f"ë‹¤ìŒì€ ë‰´ìŠ¤ ì œëª©ê³¼ ë³¸ë¬¸ì…ë‹ˆë‹¤. ì œëª©ì´ ë³¸ë¬¸ ë‚´ìš©ì„ ì¶©ë¶„íˆ ë°˜ì˜í•˜ê³  ìˆëŠ”ì§€, ì¤‘ìš”í•œ ë§¥ë½ì´ë‚˜ ì¸ë¬¼ì˜ ì…ì¥ì´ ì™œê³¡ë˜ê±°ë‚˜ ëˆ„ë½ë˜ì—ˆëŠ”ì§€ íŒë‹¨í•´ì¤˜.\n\nì œëª©: {title}\në³¸ë¬¸: {text_content}\n\në¶„ì„ ê²°ê³¼ë¥¼ ê°„ë‹¨íˆ 3~5ì¤„ë¡œ ì •ë¦¬í•´ì¤˜."
    try:
        completion = client_openai.chat.completions.create(model="gpt-4", messages=[{"role": "system", "content": "ë„ˆëŠ” ê³µì •í•œ ë‰´ìŠ¤ í”„ë ˆì´ë° ë¶„ì„ ë„ìš°ë¯¸ì•¼."}, {"role": "user", "content": prompt}])
        return completion.choices[0].message.content.strip()
    except Exception as e:
        st.warning("í”„ë ˆì´ë° ë¶„ì„ ì¤‘ ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        print(f"OpenAI í”„ë ˆì´ë° ë¶„ì„ API ì˜¤ë¥˜: {e}")
        return "í”„ë ˆì´ë° ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

def extract_keywords_gemini(article_text):
    model = genai.GenerativeModel(
        model_name='gemini-1.5-flash-latest',
        system_instruction="You are an AI assistant specialized in extracting the most important keywords from news articles. Keywords should be nouns or core noun phrases. Respond only with the keywords, separated by commas."
    )
    user_prompt = f"ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ 5ê°œë§Œ ì¶”ì¶œí•˜ì—¬, ê° í‚¤ì›Œë“œë¥¼ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•œ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ë¬¸ì¥ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n\nì˜ˆì‹œ ì‘ë‹µ:\ní‚¤ì›Œë“œ1,í•µì‹¬ ë‹¨ì–´,ì„¸ë²ˆì§¸ í‚¤ì›Œë“œ,ì¤‘ìš” ê°œë…,ë§ˆì§€ë§‰\n\nê¸°ì‚¬ ë³¸ë¬¸:\n{article_text}"
    try:
        response = model.generate_content(user_prompt, generation_config=genai.types.GenerationConfig(temperature=0.2))
        keywords_string = response.text.strip()
        if keywords_string:
            if "\n" in keywords_string: keywords_string = keywords_string.split("\n")[-1]
            if ":" in keywords_string: keywords_string = keywords_string.split(":")[-1].strip()
            return [kw.strip() for kw in keywords_string.split(',') if kw.strip()]
        return []
    except Exception as e:
        print(f"Gemini í‚¤ì›Œë“œ ì¶”ì¶œ API ì˜¤ë¥˜: {e}")
        st.warning("AI í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        return []

# --- ìœ ì‚¬ë„ ì¸¡ì • ëª¨ë¸ ë¡œë“œ (ë‹¤ì‹œ í™œì„±í™”) ---
model_similarity = None # ë³€ìˆ˜ ì´ˆê¸°í™”
try:
    model_similarity = SentenceTransformer('all-MiniLM-L6-v2', device='cpu') # device='cpu' ëª…ì‹œì  ì¶”ê°€
    if model_similarity is None: # ë¡œë“œ ì‹¤íŒ¨ ì‹œ (ë“œë¬¼ì§€ë§Œ)
        st.error("SentenceTransformer ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìœ¼ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì•± ì‹¤í–‰ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        st.stop()
except Exception as e:
    st.error(f"SentenceTransformer ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
    st.error("íŒ: ì´ ì˜¤ë¥˜ëŠ” ë³´í†µ torch ë˜ëŠ” sentence-transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜/í˜¸í™˜ì„± ë¬¸ì œì…ë‹ˆë‹¤.")
    st.info("ìœ ì‚¬ë„ ë¶„ì„ ê¸°ëŠ¥ ì—†ì´ ì•±ì„ ê³„ì† ì‚¬ìš©í•˜ì‹œë ¤ë©´ ì½”ë“œì—ì„œ í•´ë‹¹ ëª¨ë¸ ë¡œë“œ ë¶€ë¶„ì„ ë‹¤ì‹œ ì£¼ì„ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
    st.stop()

# --- ê¸°ì‚¬ ë¶„ì„ ë° ê²°ê³¼ í‘œì‹œ í•¨ìˆ˜ ---
def display_article_analysis_content(title_to_display, text_content, article_url):
    st.markdown("---")
    st.subheader("ğŸ“° ê¸°ì‚¬ ì œëª©")
    st.write(f"**{title_to_display}**")
    st.markdown(f"[ğŸ”— ê¸°ì‚¬ ì›ë¬¸ ë°”ë¡œê°€ê¸°]({article_url})", unsafe_allow_html=True)
    st.markdown("---")

    # Geminië¡œ ìš”ì•½
    st.subheader("ğŸ§¾ ë³¸ë¬¸ ìš”ì•½ (by Gemini AI)")
    with st.expander("âš ï¸ AI ìš”ì•½ì— ëŒ€í•œ ì¤‘ìš” ì•ˆë‚´ (í´ë¦­í•˜ì—¬ í™•ì¸)", expanded=False):
        st.markdown(""" **ì£¼ì˜: AI ê¸°ë°˜ ìš”ì•½ (Gemini)**\n\n* ë³¸ ìš”ì•½ì€ Gemini ëª¨ë¸ì„ í†µí•´ ìƒì„±ë˜ì—ˆìœ¼ë©°, ê¸°ì‚¬ì˜ ëª¨ë“  ë‚´ìš©ì„ ì™„ë²½í•˜ê²Œ ë°˜ì˜í•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n* AIëŠ” í•™ìŠµ ë°ì´í„°ì˜ í•œê³„ë‚˜ ìš”ì•½ ê³¼ì •ì˜ íŠ¹ì„±ìœ¼ë¡œ ì¸í•´ ë•Œë•Œë¡œ ë¶€ì •í™•í•œ ë‚´ìš©ì„ ì „ë‹¬í•˜ê±°ë‚˜ ì¤‘ìš”í•œ ë‚´ìš©ì„ ìƒëµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n* ì œê³µëœ ìš”ì•½ì€ ê¸°ì‚¬ì˜ í•µì‹¬ ë‚´ìš©ì„ ë¹ ë¥´ê²Œ íŒŒì•…í•˜ê¸° ìœ„í•œ ì°¸ê³  ìë£Œë¡œë§Œ í™œìš©í•´ì£¼ì‹­ì‹œì˜¤.\n* ê¸°ì‚¬ì˜ ì „ì²´ì ì¸ ë§¥ë½ê³¼ ì •í™•í•œ ì •ë³´ í™•ì¸ì„ ìœ„í•´ì„œëŠ” ë°˜ë“œì‹œ ì›ë¬¸ ê¸°ì‚¬ë¥¼ í•¨ê»˜ ì½ì–´ë³´ì‹œëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë©°, ìµœì¢…ì ì¸ ë‚´ìš©ì— ëŒ€í•œ íŒë‹¨ì€ ì‚¬ìš©ìì˜ ì±…ì„ì…ë‹ˆë‹¤. """)
    body_summary = summarize_text_gemini(text_content)
    st.write(body_summary)
    st.markdown("---")

    # Geminië¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¹„êµ
    st.subheader("ğŸ” AI ì¶”ì¶œ ì£¼ìš” í‚¤ì›Œë“œì™€ ì œëª© ë¹„êµ (by Gemini AI)")
    extracted_keywords = extract_keywords_gemini(text_content)
    if not extracted_keywords:
        st.info("â„¹ï¸ AIê°€ ë³¸ë¬¸ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆê±°ë‚˜, ì¶”ì¶œëœ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.caption(f"AI(Gemini)ê°€ ë³¸ë¬¸ì—ì„œ ì¶”ì¶œí•œ ì£¼ìš” í‚¤ì›Œë“œ: **{', '.join(extracted_keywords)}**")
        missing_in_title = [kw for kw in extracted_keywords if kw.lower() not in title_to_display.lower()]
        if missing_in_title:
            st.warning(f"â— AI ì¶”ì¶œ í‚¤ì›Œë“œ ì¤‘ ì¼ë¶€ê°€ ì œëª©ì— ë¹ ì ¸ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤: **{', '.join(missing_in_title)}**")
        else:
            st.success("âœ… AI ì¶”ì¶œ í•µì‹¬ í‚¤ì›Œë“œê°€ ì œëª©ì— ì˜ ë°˜ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    st.markdown("---")
    
    # ìœ ì‚¬ë„ íŒë‹¨ (ë‹¤ì‹œ í™œì„±í™”)
    st.subheader("ğŸ“Š ì œëª©-ë³¸ë¬¸ìš”ì•½ ìœ ì‚¬ë„ íŒë‹¨")
    if model_similarity is not None: # ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
        try:
            embeddings = model_similarity.encode([title_to_display, body_summary], convert_to_tensor=True)
            similarity = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()
            
            similarity_threshold_high = 0.65
            similarity_threshold_mid = 0.40
            if similarity > similarity_threshold_high: result_text, result_color = "âœ… **ë†’ìŒ**: ì œëª©ì´ ë³¸ë¬¸ ìš”ì•½ ë‚´ìš©ì„ ì˜ ë°˜ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤.", "green"
            elif similarity > similarity_threshold_mid: result_text, result_color = "ğŸŸ¡ **ì¤‘ê°„**: ì œëª©ì´ ë³¸ë¬¸ ìš”ì•½ê³¼ ë‹¤ì†Œ ê´€ë ¨ì€ ìˆì§€ë§Œ, ë‚´ìš©ì´ ì•½ê°„ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.", "orange"
            else: result_text, result_color = "âš ï¸ **ë‚®ìŒ**: ì œëª©ì´ ë³¸ë¬¸ ìš”ì•½ ë‚´ìš©ê³¼ ë§ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‚šì‹œì„±ì´ê±°ë‚˜ ë‹¤ë¥¸ ë‚´ìš©ì„ ë‹¤ë£° ê°€ëŠ¥ì„±ì„ í™•ì¸í•´ë³´ì„¸ìš”.", "red"
            
            st.markdown(f"<span style='color:{result_color};'>{result_text}</span> (ìœ ì‚¬ë„ ì ìˆ˜: {similarity:.2f})", unsafe_allow_html=True)
            st.caption(f"ì°¸ê³ : ìœ ì‚¬ë„ëŠ” ì œëª©ê³¼ AI ìš”ì•½ë¬¸ ê°„ì˜ ì˜ë¯¸ì  ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ë©°, ì„ê³„ê°’(í˜„ì¬: ë†’ìŒ {similarity_threshold_high}, ì¤‘ê°„ {similarity_threshold_mid})ì— ë”°ë¼ í•´ì„ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        except Exception as e_sim:
            st.error(f"ìœ ì‚¬ë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e_sim}")
            print(f"ìœ ì‚¬ë„ ë¶„ì„ ì˜¤ë¥˜: {e_sim}")
            st.info("â„¹ï¸ ìœ ì‚¬ë„ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("â„¹ï¸ ì œëª©-ë³¸ë¬¸ ìœ ì‚¬ë„ ë¶„ì„ ê¸°ëŠ¥ì´ SentenceTransformer ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ë¡œ ì¸í•´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    st.markdown("---")

    # GPTë¡œ í”„ë ˆì´ë° ë¶„ì„ (ìœ ì§€)
    st.subheader("ğŸ•µï¸ í”„ë ˆì´ë° ë¶„ì„ ê²°ê³¼ (by GPT)")
    with st.expander("âš ï¸ AI í”„ë ˆì´ë° ë¶„ì„ ì£¼ì˜ì‚¬í•­ (í´ë¦­í•˜ì—¬ í™•ì¸)"):
        st.markdown(""" **ì£¼ì˜: AI ê¸°ë°˜ í”„ë ˆì´ë° ë¶„ì„ (GPT)**\n\n* ë³¸ ë¶„ì„ì€ GPT ëª¨ë¸ì— ì˜í•´ ìˆ˜í–‰ë˜ì—ˆìœ¼ë©°, ì™„ë²½ì„±ì„ ë³´ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n* AIëŠ” ë°ì´í„°ì™€ í•™ìŠµ ë°©ì‹ì— ë”°ë¼ í¸í–¥ëœ ê²°ê³¼ë¥¼ ì œì‹œí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n* ì œê³µëœ ë¶„ì„ì€ ì°¸ê³  ìë£Œë¡œ í™œìš©í•˜ì‹œê³ , ìµœì¢…ì ì¸ íŒë‹¨ì€ ì‚¬ìš©ìì˜ ì±…ì„í•˜ì— ì´ë£¨ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤. """)
    framing_result = detect_bias_openai(title_to_display, text_content)
    st.info(framing_result)

# --- Streamlit ì•± UI êµ¬ì„± ---
st.set_page_config(page_title="ë‰´ìŠ¤ì½ì€ì²™ë°©ì§€ê¸° (í•˜ì´ë¸Œë¦¬ë“œ)", page_icon="ğŸ§")
st.title("ğŸ§ ë‰´ìŠ¤ì½ì€ì²™ë°©ì§€ê¸°")
st.write("í‚¤ì›Œë“œ ê²€ìƒ‰ ë˜ëŠ” URL ì§ì ‘ ì…ë ¥ìœ¼ë¡œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ AIì™€ í•¨ê»˜ ë¶„ì„í•´ë³´ì„¸ìš”!")
st.caption("ë³¸ë¬¸ ìš”ì•½ ë° í‚¤ì›Œë“œ ì¶”ì¶œì€ Gemini AI, í”„ë ˆì´ë° ë¶„ì„ì€ OpenAI GPTë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

input_tab1, input_tab2 = st.tabs(["ğŸ—‚ï¸ í‚¤ì›Œë“œ/RSSí”¼ë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰/ë¶„ì„", "ğŸ”— URL ì§ì ‘ ì…ë ¥/ë¶„ì„"])

with input_tab1:
    st.subheader("í‚¤ì›Œë“œ ë˜ëŠ” RSS í”¼ë“œ URLë¡œ ë‰´ìŠ¤ ì°¾ì•„ ë¶„ì„í•˜ê¸°")
    search_type_tab1 = st.radio( "ê²€ìƒ‰/ì…ë ¥ íƒ€ì… ì„ íƒ:", ("í‚¤ì›Œë“œë¡œ Google News ê²€ìƒ‰", "RSS í”¼ë“œ URL ì§ì ‘ ì…ë ¥"), key="search_type_tab1", horizontal=True)

    if search_type_tab1 == "í‚¤ì›Œë“œë¡œ Google News ê²€ìƒ‰":
        input_label_tab1 = "ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”:"
        input_placeholder_tab1 = "ì˜ˆ: ì• í”Œ AI ì „ëµ"
    else: 
        input_label_tab1 = "ë‰´ìŠ¤ RSS í”¼ë“œì˜ ì „ì²´ URLì„ ì…ë ¥í•˜ì„¸ìš”:"
        input_placeholder_tab1 = "ì˜ˆ: https://www.chosun.com/arc/outboundfeeds/rss/?outputType=xml"
    rss_or_keyword_input_tab1 = st.text_input(input_label_tab1, placeholder=input_placeholder_tab1, key="rss_or_keyword_input_tab1")

    if st.button("ğŸ“° ë‰´ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°", key="fetch_list_button_tab1", use_container_width=True):
        article_options_tab1 = {} 
        if not rss_or_keyword_input_tab1:
            st.warning("í‚¤ì›Œë“œ ë˜ëŠ” RSS í”¼ë“œ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            feed_url_to_parse = None # ì´ˆê¸°í™”
            if search_type_tab1 == "RSS í”¼ë“œ URL ì§ì ‘ ì…ë ¥":
                if not (rss_or_keyword_input_tab1.startswith('http://') or rss_or_keyword_input_tab1.startswith('https://')):
                    st.warning("ì˜¬ë°”ë¥¸ RSS í”¼ë“œ URL í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. 'http://' ë˜ëŠ” 'https://'ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.")
                else:
                    feed_url_to_parse = rss_or_keyword_input_tab1
                feed_source_name = "ì…ë ¥í•˜ì‹  RSS í”¼ë“œ"
            elif search_type_tab1 == "í‚¤ì›Œë“œë¡œ Google News ê²€ìƒ‰":
                feed_url_to_parse = f"https://news.google.com/rss/search?q={rss_or_keyword_input_tab1}&hl=ko&gl=KR&ceid=KR:ko"
                feed_source_name = f"'{rss_or_keyword_input_tab1}' ê´€ë ¨ Google News"

            if feed_url_to_parse: 
                try:
                    with st.spinner(f"{feed_source_name}ì—ì„œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
                        feed = feedparser.parse(feed_url_to_parse)
                    if feed.entries:
                        for entry in feed.entries[:30]: 
                            if hasattr(entry, 'title') and hasattr(entry, 'link'):
                                article_options_tab1[entry.title] = entry.link
                        if article_options_tab1:
                             st.success(f"{feed_source_name}ì—ì„œ {len(article_options_tab1)}ê±´ì˜ ê¸°ì‚¬ ì œëª©ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                        else:
                            st.warning(f"{feed_source_name}ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜, ê¸°ì‚¬ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    else:
                        st.warning(f"{feed_source_name}ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    st.error(f"{search_type_tab1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        if article_options_tab1:
            st.session_state.article_options_for_analysis_tab1 = article_options_tab1
        else: 
            if 'article_options_for_analysis_tab1' in st.session_state:
                del st.session_state.article_options_for_analysis_tab1

    if 'article_options_for_analysis_tab1' in st.session_state and st.session_state.article_options_for_analysis_tab1:
        selected_title_tab1 = st.selectbox(
            "ë¶„ì„í•  ê¸°ì‚¬ë¥¼ ì„ íƒí•˜ì„¸ìš”:",
            options=list(st.session_state.article_options_for_analysis_tab1.keys()),
            index=None,
            placeholder="ëª©ë¡ì—ì„œ ê¸°ì‚¬ë¥¼ ì„ íƒí•˜ì„¸ìš”...",
            key="selectbox_tab1"
        )
        if selected_title_tab1 and st.button("ğŸ‘† ì„ íƒí•œ ë‰´ìŠ¤ ë¶„ì„í•˜ê¸°", key="analyze_selected_button_tab1", use_container_width=True):
            url_to_analyze = st.session_state.article_options_for_analysis_tab1[selected_title_tab1]
            st.info(f"ì„ íƒí•œ ê¸°ì‚¬ ë¶„ì„ ì¤‘: {selected_title_tab1}")
            try:
                with st.spinner(f"'{selected_title_tab1}' ê¸°ì‚¬ë¥¼ ê°€ì ¸ì™€ AIê°€ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                    article = Article(url_to_analyze, language='ko') 
                    article.download()
                    article.parse()
                    if not article.title or not article.text or len(article.text) < 50:
                        st.error("ì„ íƒí•œ ê¸°ì‚¬ì˜ ì œëª©ì´ë‚˜ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆê±°ë‚˜ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
                    else:
                        display_article_analysis_content(article.title, article.text, url_to_analyze) 
            except Exception as e:
                st.error(f"ì„ íƒí•œ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

with input_tab2:
    st.subheader("URLë¡œ ì§ì ‘ ë‰´ìŠ¤ ë¶„ì„í•˜ê¸°")
    url_direct_input_tab2 = st.text_input("ë¶„ì„í•  ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì „ì²´ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”:", placeholder="ì˜ˆ: https://www.example-news.com/news/article123", key="url_direct_input_tab2")

    if st.button("ğŸš€ URL ë¶„ì„ ì‹œì‘", use_container_width=True, key="direct_url_analyze_button_tab2"):
        if not url_direct_input_tab2:
            st.warning("ë¶„ì„í•  ê¸°ì‚¬ì˜ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        elif not (url_direct_input_tab2.startswith('http://') or url_direct_input_tab2.startswith('https://')):
            st.warning("ì˜¬ë°”ë¥¸ URL í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. 'http://' ë˜ëŠ” 'https://'ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.")
        else:
            st.info(f"ì…ë ¥í•˜ì‹  URLì˜ ê¸°ì‚¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤: {url_direct_input_tab2}")
            try:
                with st.spinner(f"ê¸°ì‚¬ë¥¼ ê°€ì ¸ì™€ AIê°€ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                    article = Article(url_direct_input_tab2, language='ko')
                    article.download()
                    article.parse()
                    if not article.title or not article.text or len(article.text) < 50:
                        st.error("ê¸°ì‚¬ ì œëª©ì´ë‚˜ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆê±°ë‚˜ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ URLì„ ì‹œë„í•´ì£¼ì„¸ìš”.")
                    else:
                        display_article_analysis_content(article.title, article.text, url_direct_input_tab2)
            except Exception as e:
                st.error(f"URL ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print(f"ì „ì²´ ì˜¤ë¥˜: {e}") 
                st.caption("URLì„ í™•ì¸í•˜ì‹œê±°ë‚˜, ë‹¤ë¥¸ ê¸°ì‚¬ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”. ì¼ë¶€ ì›¹ì‚¬ì´íŠ¸ëŠ” ì™¸ë¶€ ì ‘ê·¼ì„ í†µí•œ ê¸°ì‚¬ ìˆ˜ì§‘ì„ í—ˆìš©í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")